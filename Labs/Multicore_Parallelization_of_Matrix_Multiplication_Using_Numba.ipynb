{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOPHYS 257 (Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)\n",
    "\n",
    "## Multicore Parallelization of Matrix Multiplication Using Numba\n",
    "\n",
    "In this lab we will be using Numba to accelerate matrix-matrix multiplications by exploiting parallelism. Even most laptops today have Multicore CPUs, where a *core* is a microprocessor, and each core is usually a copy of the each other core. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba](https://numba.readthedocs.io/en/stable/index.html): Documentation\n",
    "* [Numba in 30 min](https://youtu.be/DPyPmoeUdcE): Conference presentation video\n",
    "\n",
    "\n",
    "## Required Preperation\n",
    "Please watch the following videos before starting the lab (each is pretty short):\n",
    "* [Introduction to Parallel Computing](https://youtu.be/RNVIcm8-6RE)\n",
    "* [Amdahl's Law](https://youtu.be/Axx2xuB-Xuo)\n",
    "* [CPU Caching](https://youtu.be/KmairurdiaY)\n",
    "* [Pipelining](https://youtu.be/zPmfprtdzCE)\n",
    "* [Instruction Level Parallelism](https://youtu.be/ZoDUI3gTkDI)\n",
    "* [Introduction to SIMD](https://youtu.be/o_n4AKwdfiA)\n",
    "\n",
    "\n",
    "## Exercise 0\n",
    "\n",
    "Please run the following cells.  Examine the result of the example function that makes use of the *timer* wrapper. Also please use this timer wrapper (defined below) for all the exercises that follow.\n",
    "\n",
    "### Load python modules \n",
    "#### (Note: you may need to install some Python packages for the modules below, e.g. Numba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from time import time, sleep\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function-decorator for timing the runtime of your functions\n",
    "\n",
    "Below is some code that you can use to wrap your functions so that you can time them individually.  The function defined immediately below the *timer()* is an example of how to use the warpper. In the cell that follows, execute this example function. (Note, for this timer, were are making use of something called *decorators*, but a discussion about this feature is outside the scope of this class.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# defining a function decorator for timing other functions\n",
    "def mytimer(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        t_start = time() # Students look here\n",
    "        result = func(*args, **kwargs)\n",
    "        t_end = time() # Students also look here. This is how you can time things inside functions/classes\n",
    "        print(f'Function: \\'{func.__name__}({kwargs})\\' executed in {(t_end-t_start):6.5f}s\\n')\n",
    "        return result\n",
    "    return wrap\n",
    "    \n",
    "\n",
    "# Example of how to use. NOTE the \"@mytimer\" stated just above the function definition\n",
    "@mytimer\n",
    "def example_sum_timer_wrap(N):\n",
    "    \"\"\" Sum the squares of the intergers, i, contained in [1-N] \"\"\"\n",
    "    return np.sum(np.arange(1,N+1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the example function\n",
    "\n",
    "Run the example function for each of the following instances of $N = 10^5, 10^6, 10^7, 10^8$. Please examine the results, in particular, how the runtime changes with respect to $N$.\n",
    "\n",
    "\n",
    "**Answer** the following questions in the markdown cell that follow the code. \n",
    "* For each factor-of-ten increase in $N$, roughly how much longer was the runtime of the function?\n",
    "* Does this slowdown in the runtime make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'example_sum_timer_wrap({'N': 100000})' executed in 0.00106s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({'N': 1000000})' executed in 0.00971s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({'N': 10000000})' executed in 0.09520s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({'N': 100000000})' executed in 0.94551s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the example function above for each value of N (try making one-call first, then loop)\n",
    "\n",
    "N_list = 10 ** np.arange(5, 9, dtype=np.int32)\n",
    "\n",
    "for N in N_list:\n",
    "    example_sum_timer_wrap(N = N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion for Exercise 0 questions\n",
    "* For each factor-of-10 increase in $N$, the function runtime is also roughly 10 times longer.\n",
    "* This linear $O(N)$ slowdown makes sense. Within the function it creates a numpy array with size $N$, and the number of math operations also increases linearly with $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Naive matrix multiplication: \n",
    "\n",
    "For this exercise, we will write our own matrx-matrix multiplication function.  We are goint to be naive about it, so we want to loop over individual indices, instead of using slicing (in the next section, we will parallelize this code and we want to see how well Numba does at speeding up the naive code).\n",
    "\n",
    "### Tasks for this exercise\n",
    "\n",
    "1. Write a function with that calculates matrix-matrix multiplication such that $C = A\\cdot B$, where $A$, $B$, and $C$ are 2D numpy arrays. Make the dtype for the $C$ matrix the same as the dtype for $A$. Below is a stencil you can start with. Test your results for accuracy and performance against the np.dot() function. To time the np.dot() function, you can wrap it in another function and use the *mytimer* wrapper; however, please copy the code for testing the A and B dimensions into your function wrapper for the np.dot() function. This keeps the runtime comparison as a more \"apples-to-apples\" like comparison.\n",
    "\n",
    "```python\n",
    "@mytimer\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zeros \n",
    "    # and that has the appropriate dimensions such that C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in range(<val>):\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Define your functions in the cell below.\n",
    "3. Test and compare the accuracy and runtime of your functions in the next cell.\n",
    "    - Test with non-square Matrices: $A \\in \\mathbb{R}^{N\\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$. With $N = 64$, $K = 32$, and $M = 128$.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 64, 128,$ and $256$.\n",
    "    - Test the following three cases:\n",
    "        - Case-1. $A$ and $B$ **both** have dtype=np.**float32** (make sure that $C$ is also of dtype=np.**float32**)\n",
    "        - Case-2. $A$ has dtype=np.**float64**, but $B$ has dtype=np.**float32**\n",
    "        - Case-3. $A$ and $B$ **both** have dtype=np.**float64** (make sure that $C$ is also of dtype=np.**float64**)\n",
    "    - For all three case above:\n",
    "        - Calculate and show the error by computing the sum of the difference between the $C$ matrices computed from numpy.dot() and your my_naive_matmul() functions. Assume that numpy.dot() is correct\n",
    "        - Calculate and show the *speedup* that the faster function has versus the slower function.\n",
    "        - Comment on the which of the three cases is fastest, and comment on what the speedup of the fastest case is and why it is the fastest case.\n",
    "4. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "A = np.random.rand(N, K)\n",
    "B = np.random.rand(\"for-you-to-figure-out\")\n",
    "```\n",
    "<br>\n",
    "\n",
    "### Write function definitions for Exercise 1 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 1 here.\n",
    "\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    # 2. \n",
    "    # construct a 2D numpy array for matrix C, that is filled with zeros \n",
    "    # and that has the appropriate dimensions such that C=A*B is a valid \n",
    "    # equation and operation\n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    for i in range(N):\n",
    "        for k in range(K):\n",
    "            for j in range(M):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "\n",
    "def np_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is the matrix-multiplication function using numpy \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "        \n",
    "    # 2. \n",
    "    # construct a 2D numpy array for matrix C, that is filled with zeros \n",
    "    # and that has the appropriate dimensions such that C=A*B is a valid \n",
    "    # equation and operation\n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    # 3.\n",
    "    # calculate matrix-multiplication with np.dot()\n",
    "    np.dot(A, B, out=C)\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "\n",
    "def compare_matmul(C1:np.ndarray, C2:np.ndarray):\n",
    "    \"\"\" Calculate absolute error between two matrices \"\"\"\n",
    "    return np.sum(np.abs(C1 - C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "\n",
    "def test_naive_matmul(shape):\n",
    "    \"\"\" This function compares naive matrix-multiplication with np.dot() \"\"\"\n",
    "\n",
    "    # 1.\n",
    "    # Check the shape of matrix\n",
    "    if shape[0] == shape[1] and shape[1] == shape[2]:\n",
    "        print('Square matrices, N = %d' %shape[0])\n",
    "    else:\n",
    "        print('Non-square matrices, N = %d, K = %d, M = %d' %(shape[0], shape[1], shape[2]))\n",
    "        \n",
    "    # 2.\n",
    "    # Generate random matrices\n",
    "    A0 = np.random.rand(shape[0], shape[1])\n",
    "    B0 = np.random.rand(shape[1], shape[2])\n",
    "    \n",
    "    # 3.\n",
    "    # Loop over three cases with different float data types\n",
    "    for case in range(3):\n",
    "        \n",
    "        # 3.1.\n",
    "        # Float types of matrices\n",
    "        if case == 0:\n",
    "            print('\\n#1: Both A and B are float32')\n",
    "            A = A0.astype(np.float32)\n",
    "            B = B0.astype(np.float32)\n",
    "\n",
    "        elif case == 1:\n",
    "            print('\\n#2: A is float64, and B is float32')\n",
    "            A = A0.astype(np.float64)\n",
    "            B = B0.astype(np.float32)\n",
    "\n",
    "        elif case == 2:\n",
    "            print('\\n#3: Both A and B are float64')\n",
    "            A = A0.astype(np.float64)\n",
    "            B = B0.astype(np.float64)\n",
    "\n",
    "        # 3.2.\n",
    "        # Time functions: Naive matrix-multiplication and np.dot\n",
    "        t1_start = time()\n",
    "        C1 = my_naive_matmul(A, B)\n",
    "        t1_end = time()\n",
    "        t1 = t1_end - t1_start\n",
    "\n",
    "        t2_start = time()\n",
    "        C2 = np_matmul(A, B)\n",
    "        t2_end = time()\n",
    "        t2 = t2_end - t2_start\n",
    "\n",
    "        # 3.3.\n",
    "        # Calculate speed-up and accuracy\n",
    "        if (t1 >= t2):\n",
    "            print('Numpy function is faster')\n",
    "            speedup = t1 / t2\n",
    "        else:\n",
    "            print('Naive matmul is faster')\n",
    "            speedup = t2 / t1\n",
    "        \n",
    "        print('Naive runtime: %.6f s' %t1)\n",
    "        print('Numpy runtime: %.6f s' %t2)\n",
    "        print('Speed-up factor: %.3f' %speedup)\n",
    "        print('Error: %s' %compare_matmul(C1, C2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Write the test codes for Exercise 1 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-square matrices, N = 64, K = 32, M = 128\n",
      "\n",
      "#1: Both A and B are float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.169088 s\n",
      "Numpy runtime: 0.000060 s\n",
      "Speed-up factor: 2814.310\n",
      "Error: 0.0022149086\n",
      "\n",
      "#2: A is float64, and B is float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.201134 s\n",
      "Numpy runtime: 0.000084 s\n",
      "Speed-up factor: 2396.642\n",
      "Error: 3.4914293678411923e-12\n",
      "\n",
      "#3: Both A and B are float64\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.175261 s\n",
      "Numpy runtime: 0.000072 s\n",
      "Speed-up factor: 2434.099\n",
      "Error: 3.4585667663122877e-12\n"
     ]
    }
   ],
   "source": [
    "# Non-square matrices\n",
    "\n",
    "N = 64\n",
    "K = 32\n",
    "M = 128\n",
    "\n",
    "test_naive_matmul([N, K, M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrices, N = 64\n",
      "\n",
      "#1: Both A and B are float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.177340 s\n",
      "Numpy runtime: 0.000056 s\n",
      "Speed-up factor: 3151.767\n",
      "Error: 0.0027246475\n",
      "\n",
      "#2: A is float64, and B is float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.201891 s\n",
      "Numpy runtime: 0.000110 s\n",
      "Speed-up factor: 1836.859\n",
      "Error: 3.424815986363683e-12\n",
      "\n",
      "#3: Both A and B are float64\n",
      "Numpy function is faster\n",
      "Naive runtime: 0.168809 s\n",
      "Numpy runtime: 0.000071 s\n",
      "Speed-up factor: 2383.963\n",
      "Error: 3.4710012641880894e-12\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 128\n",
      "\n",
      "#1: Both A and B are float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 1.333306 s\n",
      "Numpy runtime: 0.000244 s\n",
      "Speed-up factor: 5461.221\n",
      "Error: 0.042549133\n",
      "\n",
      "#2: A is float64, and B is float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 1.558181 s\n",
      "Numpy runtime: 0.000434 s\n",
      "Speed-up factor: 3590.926\n",
      "Error: 2.772182483568031e-11\n",
      "\n",
      "#3: Both A and B are float64\n",
      "Numpy function is faster\n",
      "Naive runtime: 1.345670 s\n",
      "Numpy runtime: 0.000215 s\n",
      "Speed-up factor: 6257.370\n",
      "Error: 2.793854037008714e-11\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 256\n",
      "\n",
      "#1: Both A and B are float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 10.761670 s\n",
      "Numpy runtime: 0.000526 s\n",
      "Speed-up factor: 20452.068\n",
      "Error: 0.23772812\n",
      "\n",
      "#2: A is float64, and B is float32\n",
      "Numpy function is faster\n",
      "Naive runtime: 11.866493 s\n",
      "Numpy runtime: 0.001447 s\n",
      "Speed-up factor: 8200.969\n",
      "Error: 2.2684076839141198e-10\n",
      "\n",
      "#3: Both A and B are float64\n",
      "Numpy function is faster\n",
      "Naive runtime: 10.606133 s\n",
      "Numpy runtime: 0.000597 s\n",
      "Speed-up factor: 17765.713\n",
      "Error: 2.2596680082642706e-10\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Square matrices\n",
    "\n",
    "for N in np.array([64, 128, 256]):\n",
    "    test_naive_matmul([N, N, N])\n",
    "    print('\\n%s\\n' %(\"-\" * 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion for Exercise 1 questions\n",
    "* Numpy function np.dot() is much faster than naive matrix multiplication. With larger matrix sizes, the speedup of np.dot() becomes more significant. This is probably because with larger matrix sizes, the parallel part of the program becomes more dominant than the serial part.\n",
    "* Typically, when both matrices are in float32 type, the runtime is the shortest. This is because float32 type has less significant digits (precision) and the operation is applied to fewer bytes of data. \n",
    "* Similarly, this case has the largest speedup, but not so different from the case where both matrices are in float64 type. Potentially this is because with fewer digits, np.dot() optimization performs better.\n",
    "* An interesting feature is that if two matrices have different float types, the runtime is usually the longest and the speedup is the smallest. This can be due to that before multiplication, there is an extra step to homogenize the inputs into the same type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Using Numba to speed up matrix multiplication: \n",
    "\n",
    "For this exercise, numba to speedup our matrx-matrix multiplication function below. However, there is an interesting twist to this exercise. We will write six versions of the naive function you wrote above. One function for each of the six possible permutations of three loops used to calculate the multplication (i.e. ijk, ikj, jik, etc.)\n",
    "\n",
    "### The tasks for this exercise:\n",
    "1. Make six copies your code for the my_naive_matmul(), one copy for each of the possible permutations and define them in the cell bellow:\n",
    "    - One of these function will have the same loop order as the my_naive_matmul() function. \n",
    "    - However name these functions: numba_mul_\\<perm\\>(), where \\<perm\\> should be replaced by the specific loop order of the function.\n",
    "2. In the cell that follows your function definitions, test and compare the accuracy and runtime of your functions against the numpy.dot() function.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 128, 256,$ and $512$.\n",
    "    - For each matrix set their dtype as: dtype=np.float64\n",
    "    - Calculate and show the error between your functions and the numpy.dot() function. (Same as in Exercise 1.)\n",
    "    - Calculate and show the *speedup* that the fastest function has versus all the other functions.\n",
    "    - You should notice that one of your permutation functions is faster than the others. For this case show the following:\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has versus the my_naive_matmul().\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has when $A$, $B$, and $C$ are all of dtype=np.float64 vs all are of dtype=np.float32.\n",
    "3. Create your matrices using random numbers. (Same as in Exercise 1.) \n",
    "4. For each function, you need to add a function decorator for numba. Numba will *JIT* the function (**J**ust **I**n **T**ime compilation). \n",
    "    - Use the flag to keep a \"cache\" of the compiled code (not to be confused with CPU cache). **Note**: to get accurate timings, you will need to run your tests **twice**, because during the first run, the code is compiled and this compile time will be included in your runtime. After the first run, the compiled binary will be stored, so consecutive runs will be faster.\n",
    "    - Use the flag to use *fast math*\n",
    "    - Use the flag to disable the Python Global Interpretor Lock (GIL).\n",
    "    - The code below should get you started, but it is incomplete.\n",
    "    \n",
    "```python   \n",
    "    \n",
    "@mytimer\n",
    "@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "def numba_mul_<perm>(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in numba.prange(<val>): # !!!! LOOK HERE !!!!\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "    \n",
    "5. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the questions asked in the markdown cell.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Write function definitions for Exercise 2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 2 here.\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_1(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: ijk (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for i in numba.prange(N):\n",
    "        for j in range(M):\n",
    "            for k in range(K):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_2(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: ikj (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for i in numba.prange(N):\n",
    "        for k in range(K):\n",
    "            for j in range(M):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_3(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: jik (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for j in numba.prange(M):\n",
    "        for i in range(N):\n",
    "            for k in range(K):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_4(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: jki (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for j in numba.prange(M):\n",
    "        for k in range(K):\n",
    "            for i in range(N):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_5(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: kij (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for k in numba.prange(K):\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True)\n",
    "def numba_matmul_6(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\"  \n",
    "    \"\"\" Index order: kji (i loops over N, j loops over M, k loops over K)\"\"\"\n",
    "    \n",
    "    N, K = A.shape\n",
    "    _K, M = B.shape\n",
    "    \n",
    "    if K != _K:\n",
    "        raise ValueError('Matrices dimensions do not match for multiplication!')\n",
    "    \n",
    "    C = np.zeros((N, M), dtype=A.dtype)\n",
    "    \n",
    "    for k in numba.prange(K):\n",
    "        for j in range(M):\n",
    "            for i in range(N):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "\n",
    "def test_numba_matmul(shape, dtype=np.float64):\n",
    "    \"\"\" This function compares numba matrix-multiplication with np.dot() \"\"\"\n",
    "    \"\"\" Numba functions have 6 permutations of indices \"\"\"\n",
    "\n",
    "    # 1.\n",
    "    # Check the shape of matrix\n",
    "    if shape[0] == shape[1] and shape[1] == shape[2]:\n",
    "        print('Square matrices, N = %d' %shape[0])\n",
    "    else:\n",
    "        print('Non-square matrices, N = %d, K = %d, M = %d' %(shape[i] for i in range(3)))\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    # 2.\n",
    "    # Generate random matrices\n",
    "    A = np.random.rand(shape[0], shape[1]).astype(dtype)\n",
    "    B = np.random.rand(shape[1], shape[2]).astype(dtype)\n",
    "    \n",
    "    # 3. \n",
    "    # Reference result from np.dot()\n",
    "    t_start = time()\n",
    "    C_ref = np_matmul(A, B)\n",
    "    t_end = time()\n",
    "    t_ref = t_end - t_start\n",
    "\n",
    "    # 4.\n",
    "    # Loop over all numba functions. Record runtime with respect to np.dot()\n",
    "    numba_func_list = [numba_matmul_1, numba_matmul_2, numba_matmul_3, \\\n",
    "                       numba_matmul_4, numba_matmul_5, numba_matmul_6]\n",
    "    Nfunc = len(numba_func_list)\n",
    "    numba_runtime = np.zeros((Nfunc, 1))\n",
    "\n",
    "    for i in range(Nfunc):\n",
    "        \n",
    "        t_start = time()\n",
    "        C = numba_func_list[i](A, B)\n",
    "        t_end = time()\n",
    "        \n",
    "        numba_runtime[i] = t_end - t_start\n",
    "    \n",
    "    # 5.\n",
    "    # Output error\n",
    "    print('Error: %s' %(compare_matmul(C, C_ref)))\n",
    "    print()\n",
    "    \n",
    "    # 6.\n",
    "    # Find the fastest numba method and compare with np.dot()\n",
    "    ind_best = np.argmin(numba_runtime)\n",
    "    numba_runtime_best = numba_runtime[ind_best]\n",
    "    \n",
    "    if (numba_runtime_best >= t_ref):\n",
    "        print('Fastest method: np.dot()')\n",
    "        print('Best Numba permutation: %s' %(numba_func_list[ind_best].__name__))\n",
    "        print('Speed-up over Numba functions: %s' %(\", \".join(\"%.2f\" % obj for obj in numba_runtime / t_ref)))\n",
    "    else:\n",
    "        print('Fastest method: %s' %(numba_func_list[ind_best].__name__))\n",
    "        print('Speed-up over np.dot: %s' %(t_ref / numba_runtime_best))\n",
    "        print('Speed-up over Numba functions: %s' %(\", \".join(\"%.2f\" % obj for obj in numba_runtime / numba_runtime_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "\n",
    "def test_numba_naive(shape, numba_func, dtype=np.float64):\n",
    "    \"\"\" This function compares naive matrix-multiplication with Numba permutation function \"\"\"\n",
    "\n",
    "    # 1.\n",
    "    # Check the shape of matrix\n",
    "    if shape[0] == shape[1] and shape[1] == shape[2]:\n",
    "        print('Square matrices, N = %d' %shape[0])\n",
    "    else:\n",
    "        print('Non-square matrices, N = %d, K = %d, M = %d' %(shape[0], shape[1], shape[2]))\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    # 2.\n",
    "    # Generate random matrices\n",
    "    A = np.random.rand(shape[0], shape[1])\n",
    "    B = np.random.rand(shape[1], shape[2])\n",
    "    \n",
    "    # 3.\n",
    "    # Time functions: Naive matrix-multiplication and Numba function\n",
    "    t1_start = time()\n",
    "    C1 = my_naive_matmul(A, B)\n",
    "    t1_end = time()\n",
    "    t1 = t1_end - t1_start\n",
    "\n",
    "    t2_start = time()\n",
    "    C2 = numba_func(A, B)\n",
    "    t2_end = time()\n",
    "    t2 = t2_end - t2_start\n",
    "\n",
    "    # 4.\n",
    "    # Calculate speed-up and accuracy\n",
    "    if (t1 >= t2):\n",
    "        print('%s is faster' %(numba_func.__name__))\n",
    "        speedup = t1 / t2\n",
    "    else:\n",
    "        print('Naive matmul is faster')\n",
    "        speedup = t2 / t1\n",
    "\n",
    "    print('Speed-up factor: %.3f' %speedup)\n",
    "    print('Error: %s' %compare_matmul(C1, C2))\n",
    "    \n",
    "\n",
    "def test_numba_float_type(shape, numba_func):\n",
    "    \"\"\" This function compares Numba permutation function for different data types \"\"\"\n",
    "\n",
    "    # 1.\n",
    "    # Check the shape of matrix\n",
    "    if shape[0] == shape[1] and shape[1] == shape[2]:\n",
    "        print('Square matrices, N = %d' %shape[0])\n",
    "    else:\n",
    "        print('Non-square matrices, N = %d, K = %d, M = %d' %(shape[0], shape[1], shape[2]))\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    # 2.\n",
    "    # Generate random matrices\n",
    "    A = np.random.rand(shape[0], shape[1])\n",
    "    B = np.random.rand(shape[1], shape[2])\n",
    "    \n",
    "    # 3.\n",
    "    # Time functions: Numba function with different data types\n",
    "    t1_start = time()\n",
    "    C1 = numba_func(A.astype(np.float64), B.astype(np.float64))\n",
    "    t1_end = time()\n",
    "    t1 = t1_end - t1_start\n",
    "\n",
    "    t2_start = time()\n",
    "    C2 = numba_func(A.astype(np.float32), B.astype(np.float32))\n",
    "    t2_end = time()\n",
    "    t2 = t2_end - t2_start\n",
    "\n",
    "    # 4.\n",
    "    # Calculate speed-up and accuracy\n",
    "    if (t1 >= t2):\n",
    "        print('Float32 is faster')\n",
    "        speedup = t1 / t2\n",
    "    else:\n",
    "        print('Float64 is faster')\n",
    "        speedup = t2 / t1\n",
    "\n",
    "    print('Speed-up factor: %.3f' %speedup)\n",
    "    print('Difference in results: %s' %compare_matmul(C1, C2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Write the test codes for Exercise 2 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrices, N = 128\n",
      "\n",
      "Error: 0.0\n",
      "\n",
      "Fastest method: np.dot()\n",
      "Best Numba permutation: numba_matmul_2\n",
      "Speed-up over Numba functions: 16.81, 2.80, 13.90, 23.98, 9.56, 24.03\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 256\n",
      "\n",
      "Error: 0.0\n",
      "\n",
      "Fastest method: np.dot()\n",
      "Best Numba permutation: numba_matmul_2\n",
      "Speed-up over Numba functions: 46.65, 6.10, 41.91, 118.17, 24.92, 120.02\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 512\n",
      "\n",
      "Error: 1.6109439116007707e-08\n",
      "\n",
      "Fastest method: np.dot()\n",
      "Best Numba permutation: numba_matmul_2\n",
      "Speed-up over Numba functions: 102.49, 12.33, 104.42, 295.45, 42.95, 289.81\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare with np.dot()\n",
    "\n",
    "for N in np.array([128, 256, 512]):\n",
    "    test_numba_matmul([N, N, N], dtype=np.float64)\n",
    "    print('\\n%s\\n' %(\"-\" * 75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrices, N = 128\n",
      "\n",
      "numba_matmul_2 is faster\n",
      "Speed-up factor: 2574.797\n",
      "Error: 2.7686297698892304e-11\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 256\n",
      "\n",
      "numba_matmul_2 is faster\n",
      "Speed-up factor: 2726.335\n",
      "Error: 2.1925217197349411e-10\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 512\n",
      "\n",
      "numba_matmul_2 is faster\n",
      "Speed-up factor: 2231.254\n",
      "Error: 1.7816859099184512e-09\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fastest Numba permutation function compared with naive matrix-multiplication\n",
    "\n",
    "for N in np.array([128, 256, 512]):\n",
    "    test_numba_naive([N, N, N], numba_matmul_2, dtype=np.float64)\n",
    "    print('\\n%s\\n' %(\"-\" * 75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square matrices, N = 128\n",
      "\n",
      "Float32 is faster\n",
      "Speed-up factor: 1.053\n",
      "Difference in results: 0.06564263418561822\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 256\n",
      "\n",
      "Float32 is faster\n",
      "Speed-up factor: 1.036\n",
      "Difference in results: 0.7351486192010128\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Square matrices, N = 512\n",
      "\n",
      "Float32 is faster\n",
      "Speed-up factor: 1.008\n",
      "Difference in results: 8.168771339268773\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fastest Numba permutation function, compare different float types\n",
    "\n",
    "for N in np.array([128, 256, 512]):\n",
    "    test_numba_float_type([N, N, N], numba_matmul_2)\n",
    "    print('\\n%s\\n' %(\"-\" * 75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Discussion for Exercise 2\n",
    "1. What do you think is causing the differences in performance between the various permutations?\n",
    "1. Which function is fastest (permutations or numpy.dot)? Why do you think this function is the fastest, and could there be multiple factors involved regarding the superior performance?\n",
    "1. When comparing the matrix-matrix performance between the cases were all matrices had dtype=np.float64 vs.  all matrices having dtype=np.float32, which dtype was fastest? Roughly what was the speedup when using this dtype vs the other?\n",
    "1. Does Amdahl's Law play a major factor in the performance differences? Which part of the matrix-matrix multiplication was not parallelized (serial portion)? (Hint: which matrices did we reuse for each function?) Could we parallelize this part, and if so, are there caveats?\n",
    "1. Do you think all codes should be parallelized? What about matrix-matrix multiplication? (This is a subjective question, but I am looking for a brief, but rational and informed arguement).\n",
    "1. For those taking the class for **4 units**: Regarding the performance differences, which Law do you think is more relevant when comparing the performance differences between each of the functions in this exercise, Amdahl's Law or Gustafson's Law? Explain your reasoning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "862d5657",
   "metadata": {},
   "source": [
    "## Answer for Exercise 2\n",
    "\n",
    "1. The CPU cache management should cause the differences in performance. For example, the second case, which is the fastest one, has three loops organized in this way:\n",
    "```python\n",
    "for i in numba.prange(N):\n",
    "    for k in range(K):\n",
    "        for j in range(M):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "```\n",
    "Under this case, each matrix is accessed based on the order where the second index is in the inner loop compared to the first index. This is consistent with the 1-D array representation of the matrix, and the cache memory is best used since the new element is always near the previous one. By contrast, for the slowest permutations like $(k, j, i)$ and $(j, k, i)$, the order of indices is contradictory to the one stored in cache, so the cache keeps refreshing the storage.\n",
    "\n",
    "2. np.dot() is the fastest, because it uses BLAS library (Basic Linear Algebra Subprograms), which contains highly optimized algorithms for vector and matrix operations.\n",
    "\n",
    "3. Float32 type yields slightly faster computation than float64 type. The speedup factor is just around $1.01$-$1.05$, and appears to be decreasing as the matrix size $N$ goes up.\n",
    "\n",
    "4. The speedup factor for the fastest permutation function using Numba versus my_naive_matmul() is over $2000$. This indicates that the parallel portion of the program is significant. Based on Amdahl's law, assuming $1/(1-p) = 2000$, the parallel portion of the program is $p = 99.95\\%$. This is reasonable, as the original my_naive_matmul() has time complexity $O(N^3)$, and we have quite large matrix size $N$. On the other hand, as the matrix size $N$ goes up, the speedup factor is basically the same. From Amdahl's law, this suggests that the parallel portion $p$ is less influenced by matrix size $N$, given the processor number unchanged. Gustafson's law yields the same conclusion that $p$ does not grow as the problem scales, within the exercise range. The serial portion of the function should include: matrix dimension check, creating zero matrix $C$, and adding results to corresponding entries in $C$ after the element multiplication is done.\n",
    "\n",
    "5. Not all codes need to be parallelized. First, only the codes that contain parallel portion can be parallelized. A completely serial procedure can not be improved by having more processors, constrained by the order of operations. Second, the amount of parallel portion $p$ matters. Amdahl's law indicates that the maximum speedup factor is $1/(1-p)$. If the serial portion is too large, or the code already optimizes the major parallel portion, then probably you don't need to further make it parallelized. Third, coding time needs to be considered. Widely used subroutines should be parallelized with higher priority than less-called subroutines. For example, matrix multiplication is a basic step in linear algebra that is widely applied in many scientific areas. Therefore, we should consider to parallelize and optimize matrix multiplication, because we use it many times everyday.\n",
    "\n",
    "6. (Extra) Both laws can be applied to analyze the speedup, since the processor number is never changed (i.e. always on my laptop). Generally, if the problem size is fixed and we study the speedup as a function of processor number, Amdahl's law is relevant. If the problem size (matrix size here) also scales with increasing number of processors, then Gustafson's law is more relevant.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
